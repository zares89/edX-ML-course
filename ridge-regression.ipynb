{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-based solver for ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will create a **gradient descent** solver for **ridge regression** and then compare it to the built-in solver in `sklearn.linear_model`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up notebook and create data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading in some standard packages, we create a synthetic data set consisting of data points `(x,y)`:\n",
    "* `x`: 100-dimensional vector whose coordinates are independent draws from a standard normal (Gaussian) distribution\n",
    "* `y`: response value given by `y = wx + e` where `w` is a target regression function and `e` is Gaussian noise\n",
    "\n",
    "We will fix `w` to be the 100-dimensional vector whose first ten coordinates are exactly 1.0, and whose remaining coordinates are zero. Thus only the first ten coordinates of `x` are relevant to the regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following procedure, **generate_data**, creates a data set of a specified number of points. It is invoked as follows:\n",
    "* `trainx, trainy = generate_data(n)`\n",
    "\n",
    "Here:\n",
    "* `n` is the target number of points\n",
    "* `trainx`: `nx100` array of data points\n",
    "* `trainy`: array of `n` response values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n):\n",
    "    d = 100\n",
    "    w = np.zeros(d)\n",
    "    for i in range(0,10):\n",
    "        w[i] = 1.0\n",
    "    #\n",
    "    trainx = np.random.normal(size=(n,d))\n",
    "    e = np.random.normal(size=(n))\n",
    "    trainy = np.dot(trainx, w) + e\n",
    "    #\n",
    "    return trainx, trainy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient descent solver for ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">**For you to do:**</font> Define a procedure, **ridge_regression_GD**, that uses gradient descent to solve the ridge regression problem. It is invoked as follows:\n",
    "\n",
    "* `w,b,losses = ridge_regression_GD(x,y,C)`\n",
    "\n",
    "Here, the input consists of:\n",
    "* training data `x,y`, where `x` and `y` are numpy arrays of dimension `n`-by-`d` and `n`, respectively (if there are `n` training points)\n",
    "* regularization constant `C`\n",
    "\n",
    "The function should find the `d`-dimensional vector `w` and offset `b` that minimize the ridge regression loss function (with regularization constant `C`), and return:\n",
    "* `w` and `b`\n",
    "* `losses`, an array containing the ridge regression loss at each iteration\n",
    "\n",
    "<font color=\"magenta\">Advice:</font> First figure out the derivative, which has a relatively simple form. Next, when implementing gradient descent, think carefully about two issues.\n",
    "\n",
    "1. What is the step size?\n",
    "2. When has the procedure converged?\n",
    "\n",
    "Take the time to experiment with different ways of handling these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_GD(x,y,C):\n",
    "    ### Put your code here\n",
    "    w_size = x.shape[1]\n",
    "    w_old = np.zeros(w_size + 1)\n",
    "    col_ones = np.ones((x.shape[0], 1))\n",
    "    xp = np.append(col_ones, x, axis=1) # Add a column of ones to x\n",
    "    tol = 1.e-4\n",
    "    eta = 0.002\n",
    "    \n",
    "    # Initialize the weight vector\n",
    "    for i in range(11):\n",
    "        w_old[i] = 1.0  \n",
    "    w_old_squared = np.dot(w_old[1:], w_old[1:])\n",
    "    # Calculating the loss function\n",
    "    losses_old = 0\n",
    "    for i in range(y.shape[0]):\n",
    "        losses_old += (y[i] - np.dot(w_old, xp[i]))**2 + C * w_old_squared\n",
    "    \n",
    "    print(\"Current loss amount is: \", losses_old)\n",
    "    step = 0\n",
    "    max_step = 100\n",
    "    losses = []\n",
    "    while True:\n",
    "        step += 1  \n",
    "        w_old_squared = np.dot(w_old[1:], w_old[1:])\n",
    "        loss_gradient = np.zeros(w_old.shape[0])\n",
    "        w_new = np.zeros(w_size + 1)\n",
    "        for j in range(w_old.shape[0]):\n",
    "            sum_linear = 0\n",
    "            for i in range(y.shape[0]):\n",
    "                sum_linear -= 2 * (y[i] - np.dot(w_old, xp[i][:])) * xp[i][j]\n",
    "            w_new[j] = w_old[j] - eta * (sum_linear + 2 * C * w_old[j])\n",
    "            if j == 1:\n",
    "                loss_gradient[j] = sum_linear   \n",
    "            else:\n",
    "                loss_gradient[j] = sum_linear + 2 * C * w_old[j]\n",
    "                \n",
    "            loss_gradient_norm = np.linalg.norm(loss_gradient)\n",
    "        print(\"The norm of loss gradient is \", loss_gradient_norm)\n",
    "    #    if step >= 2:\n",
    "    #        angle_grad = np.arccos(np.dot(loss_gradient, loss_gradient_old) / (np.linalg.norm(loss_gradient_old) * loss_gradient_norm))\n",
    "    #        print(\"Angle between the gradients is: \", angle_grad)\n",
    "        w_new_squared = np.dot(w_new[1:], w_new[1:])\n",
    "        \n",
    "        losses_new = 0\n",
    "        for i in range(y.shape[0]):\n",
    "            losses_new += (y[i] - np.dot(w_new, xp[i]))**2\n",
    "        losses_new += C * w_new_squared\n",
    "        print(\"Future loss amount is: \", losses_new)\n",
    "        losses.append(losses_new)\n",
    "        if step == max_step:\n",
    "            w = w_new[1:]\n",
    "            b = w_new[0]\n",
    "            \n",
    "            return w,b,losses\n",
    "        elif losses_new > losses_old:\n",
    "            print(\"new loss > old loss\")\n",
    "            eta *= 0.5\n",
    "        #    print(\"w old is : \", w_old[:15])\n",
    "        #    print(\"w new is: \", w_new[:15])\n",
    "        elif losses_new < losses_old:\n",
    "            print(\"new loss < old loss, we can increase eta\")\n",
    "        #    print(\"w old is : \", w_old[:15])\n",
    "        #    print(\"w new is: \", w_new[:15])\n",
    "        #    eta *= 2.0\n",
    "        losses_old = losses_new.copy()\n",
    "        w_old = w_new.copy()\n",
    "        loss_gradient_old = loss_gradient.copy()\n",
    "        print(10 * \"*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out and print a graph of the loss values during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss amount is:  2420.8924655557103\n",
      "The norm of loss gradient is  547.7890103611779\n",
      "Future loss amount is:  226.44458612312013\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  317.7185022925719\n",
      "Future loss amount is:  189.52373290870972\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  257.32627906855066\n",
      "Future loss amount is:  175.25328051702286\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  232.17626244262686\n",
      "Future loss amount is:  169.7604353249667\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  226.55289091747215\n",
      "Future loss amount is:  169.8545396820483\n",
      "new loss > old loss\n",
      "**********\n",
      "The norm of loss gradient is  239.45176187758832\n",
      "Future loss amount is:  142.48752955910513\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  33.88699673448452\n",
      "Future loss amount is:  141.70184751573612\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  21.986022772175623\n",
      "Future loss amount is:  141.28012566283132\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  19.63540911382308\n",
      "Future loss amount is:  140.9424346922317\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.779114878302334\n",
      "Future loss amount is:  140.66800193826754\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  16.152121350670757\n",
      "Future loss amount is:  140.443623698431\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  14.72174746219808\n",
      "Future loss amount is:  140.2592121611694\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  13.457627005385904\n",
      "Future loss amount is:  140.1069481490281\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  12.336047603984962\n",
      "Future loss amount is:  139.98071090986568\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  11.33746338374709\n",
      "Future loss amount is:  139.87566733329524\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  10.445744600596411\n",
      "Future loss amount is:  139.7879710354788\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  9.64745094681996\n",
      "Future loss amount is:  139.71453926223836\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  8.931294201903528\n",
      "Future loss amount is:  139.65288558729765\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  8.287719028141824\n",
      "Future loss amount is:  139.60099311279632\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  7.708576390072303\n",
      "Future loss amount is:  139.55721741154446\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  7.186866763824806\n",
      "Future loss amount is:  139.52021155029317\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  6.716536720795983\n",
      "Future loss amount is:  139.48886767748283\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  6.292316570532589\n",
      "Future loss amount is:  139.4622711591959\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  5.909589858755372\n",
      "Future loss amount is:  139.43966430840743\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  5.564287801029538\n",
      "Future loss amount is:  139.42041751160065\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  5.252803424716511\n",
      "Future loss amount is:  139.40400610519563\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  4.971921444701052\n",
      "Future loss amount is:  139.38999175446557\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  4.718760825395111\n",
      "Future loss amount is:  139.37800738252432\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  4.490727665260136\n",
      "Future loss amount is:  139.3677449162908\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  4.28547654196885\n",
      "Future loss amount is:  139.35894528088622\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  4.100878822971635\n",
      "Future loss amount is:  139.35139019841634\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  3.934996714000271\n",
      "Future loss amount is:  139.34489544202626\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  3.7860620157505904\n",
      "Future loss amount is:  139.3393052690703\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  3.652458709522001\n",
      "Future loss amount is:  139.33448781369057\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  3.5327086136339148\n",
      "Future loss amount is:  139.33033126307652\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  3.425459456785228\n",
      "Future loss amount is:  139.32674067616534\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  3.3294748104335143\n",
      "Future loss amount is:  139.3236353307302\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  3.243625413940285\n",
      "Future loss amount is:  139.32094650637697\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  3.1668815145926743\n",
      "Future loss amount is:  139.3186156281589\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  3.0983059284406007\n",
      "Future loss amount is:  139.31659270929788\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  3.037047604792521\n",
      "Future loss amount is:  139.31483504257938\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.9823355447463693\n",
      "Future loss amount is:  139.31330609895394\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.9334729804031765\n",
      "Future loss amount is:  139.31197459914074\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.8898317655653574\n",
      "Future loss amount is:  139.31081372995413\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.850846960914136\n",
      "Future loss amount is:  139.3098004819052\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.8160116180006676\n",
      "Future loss amount is:  139.30891508860253\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.784871778550224\n",
      "Future loss amount is:  139.30814055173107\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.757021710563738\n",
      "Future loss amount is:  139.30746223808097\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.7320994025110905\n",
      "Future loss amount is:  139.3068675373163\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.709782333353637\n",
      "Future loss amount is:  139.3063455710162\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.689783530725452\n",
      "Future loss amount is:  139.30588694504976\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.6718479234789396\n",
      "Future loss amount is:  139.30548353861846\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.65574898880356\n",
      "Future loss amount is:  139.3051283243561\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.6412856887342935\n",
      "Future loss amount is:  139.30481521476864\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.6282796863959352\n",
      "Future loss amount is:  139.30453893103027\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.6165728288493004\n",
      "Future loss amount is:  139.3042948907755\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.606024880929662\n",
      "Future loss amount is:  139.30407911204938\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.596511492872842\n",
      "Future loss amount is:  139.30388813101115\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5879223837176313\n",
      "Future loss amount is:  139.30371893135825\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5801597222797077\n",
      "Future loss amount is:  139.3035688837457\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5731366877931094\n",
      "Future loss amount is:  139.30343569373784\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.566776192978074\n",
      "Future loss amount is:  139.30331735704885\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of loss gradient is  2.5610097532005467\n",
      "Future loss amount is:  139.30321212101785\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5557764864658874\n",
      "Future loss amount is:  139.3031184514171\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5510222301465735\n",
      "Future loss amount is:  139.30303500382956\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5466987615300254\n",
      "Future loss amount is:  139.3029605989457\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.542763110463202\n",
      "Future loss amount is:  139.3028942012187\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.539176953503388\n",
      "Future loss amount is:  139.30283490040856\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.535906080072801\n",
      "Future loss amount is:  139.3027818956052\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5329199221246004\n",
      "Future loss amount is:  139.3027344813869\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5301911397564827\n",
      "Future loss amount is:  139.30269203581645\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5276952560695944\n",
      "Future loss amount is:  139.30265401001918\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.525410335329602\n",
      "Future loss amount is:  139.3026199191292\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.52331669919517\n",
      "Future loss amount is:  139.30258933441215\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5213966763936377\n",
      "Future loss amount is:  139.30256187640967\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.51963438178107\n",
      "Future loss amount is:  139.3025372089617\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5180155212146524\n",
      "Future loss amount is:  139.30251503399413\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5165272191082058\n",
      "Future loss amount is:  139.30249508696582\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5151578659147904\n",
      "Future loss amount is:  139.30247713288966\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5138969831310534\n",
      "Future loss amount is:  139.30246096285035\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.512735103709016\n",
      "Future loss amount is:  139.302446390955\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5116636660229683\n",
      "Future loss amount is:  139.30243325165907\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5106749197719505\n",
      "Future loss amount is:  139.3024213974207\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5097618423956067\n",
      "Future loss amount is:  139.30241069663853\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.508918064757607\n",
      "Future loss amount is:  139.30240103184025\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.508137805005522\n",
      "Future loss amount is:  139.30239229808697\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.507415809650653\n",
      "Future loss amount is:  139.30238440156754\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.50674730102459\n",
      "Future loss amount is:  139.30237725835937\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.506127930380217\n",
      "Future loss amount is:  139.30237079333412\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5055537359854543\n",
      "Future loss amount is:  139.30236493919148\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.505021105640582\n",
      "Future loss amount is:  139.30235963560426\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5045267431231264\n",
      "Future loss amount is:  139.3023548284611\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.504067638113533\n",
      "Future loss amount is:  139.30235046919742\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5036410392168427\n",
      "Future loss amount is:  139.3023465142002\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5032444297396146\n",
      "Future loss amount is:  139.30234292427997\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.502875505914743\n",
      "Future loss amount is:  139.3023396642037\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5025321573148904\n",
      "Future loss amount is:  139.30233670227682\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5022124492110493\n",
      "Future loss amount is:  139.30233400997372\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.501914606674229\n",
      "Future loss amount is:  139.3023315616073\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  2.5016370002314114\n",
      "Future loss amount is:  139.30232933403494\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate 200 data points\n",
    "n = 200\n",
    "#x,y = generate_data(n)\n",
    "# Set regularization constant\n",
    "C = 1.0\n",
    "# Run gradient descent solver\n",
    "w, b, losses = ridge_regression_GD(x,y,C)\n",
    "# Plot the losses\n",
    "plt.plot(losses,'r')\n",
    "plt.xlabel('Iterations', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">**Something to think about**</font>\n",
    "\n",
    "1. In setting the step size, does it work to use a fixed schedule 1/t? Why or why not?\n",
    "\n",
    "2. Can you set up the gradient descent procedure in such a way that on each iteration, the loss monotonically decreases?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate the gradient descent solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the regressor found by your gradient descent procedure to that returned by the built-in ridge regression solver in `sklearn`. We will compare them in two ways:\n",
    "* Their MSE values\n",
    "* The distance between the corresponding `w`-vectors\n",
    "\n",
    "The latter should be smaller than 10^{-4}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(w,b,x,y):\n",
    "    residuals = y - (np.dot(x, w) + b)\n",
    "    return np.dot(residuals, residuals)/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current loss amount is:  20405.574172203003\n",
      "The norm of loss gradient is  591.3056019589648\n",
      "Future loss amount is:  310.84101177539407\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  397.34008836477665\n",
      "Future loss amount is:  283.3206709668751\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  388.969564496511\n",
      "Future loss amount is:  284.33828566552415\n",
      "new loss > old loss\n",
      "**********\n",
      "The norm of loss gradient is  406.16897068890444\n",
      "Future loss amount is:  203.80231180362523\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  56.216027216885\n",
      "Future loss amount is:  201.70075531699985\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  38.06038514701872\n",
      "Future loss amount is:  200.51727493132216\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  34.296513899233474\n",
      "Future loss amount is:  199.62030066582105\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  31.046288554361926\n",
      "Future loss amount is:  198.92839873842678\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  28.455900447476655\n",
      "Future loss amount is:  198.3895524306011\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  26.353774619026655\n",
      "Future loss amount is:  197.96634855638013\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  24.652441816752567\n",
      "Future loss amount is:  197.63142711860212\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  23.275761687622534\n",
      "Future loss amount is:  197.3645207089652\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  22.162866995088248\n",
      "Future loss amount is:  197.15045068080133\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  21.264008129586273\n",
      "Future loss amount is:  196.97773799409532\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  20.5384999698615\n",
      "Future loss amount is:  196.83762457382758\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  19.953090541395095\n",
      "Future loss amount is:  196.72337424962498\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  19.480684623489655\n",
      "Future loss amount is:  196.62976730261312\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  19.099287592162753\n",
      "Future loss amount is:  196.55273102540914\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  18.79111396833217\n",
      "Future loss amount is:  196.4890670570397\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  18.541830866748274\n",
      "Future loss amount is:  196.43624836006262\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  18.33991893924683\n",
      "Future loss amount is:  196.39226682760489\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  18.176137579515746\n",
      "Future loss amount is:  196.3555180388991\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  18.043082154808488\n",
      "Future loss amount is:  196.324713501075\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.934821184792355\n",
      "Future loss amount is:  196.29881338506894\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.846601696115606\n",
      "Future loss amount is:  196.27697465129194\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.774611701838023\n",
      "Future loss amount is:  196.2585108089454\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.71578982548226\n",
      "Future loss amount is:  196.2428605246709\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.667673352533996\n",
      "Future loss amount is:  196.22956300253676\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.628277298316984\n",
      "Future loss amount is:  196.21823857466114\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.595998323759556\n",
      "Future loss amount is:  196.2085733232764\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.56953844812926\n",
      "Future loss amount is:  196.2003068382473\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.547844474097918\n",
      "Future loss amount is:  196.1932224255522\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.530059852574954\n",
      "Future loss amount is:  196.187139241083\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.515486383229565\n",
      "Future loss amount is:  196.18190594407614\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.503553688568008\n",
      "Future loss amount is:  196.17739555552583\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.493794833889286\n",
      "Future loss amount is:  196.17350127638485\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.48582681090118\n",
      "Future loss amount is:  196.1701330735845\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.479334875860555\n",
      "Future loss amount is:  196.16721488289696\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.474059948120825\n",
      "Future loss amount is:  196.1646823093679\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.46978844379898\n",
      "Future loss amount is:  196.16248073069215\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.4663440516787\n",
      "Future loss amount is:  196.16056372812878\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.463581062213155\n",
      "Future loss amount is:  196.15889178462945\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.46137894181349\n",
      "Future loss amount is:  196.15743120171825\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45963790840024\n",
      "Future loss amount is:  196.156153196042\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45827531428789\n",
      "Future loss amount is:  196.15503314395588\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.457222681888645\n",
      "Future loss amount is:  196.1540499484401\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.456423268779755\n",
      "Future loss amount is:  196.15318550739624\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.455830063221327\n",
      "Future loss amount is:  196.15242426618153\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.455404130641398\n",
      "Future loss amount is:  196.1517528403138\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.455113247035918\n",
      "Future loss amount is:  196.15115969677024\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.454930767513574\n",
      "Future loss amount is:  196.15063488432017\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45483468802373\n",
      "Future loss amount is:  196.15016980498024\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45480686615428\n",
      "Future loss amount is:  196.1497570200227\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.454832373203004\n",
      "Future loss amount is:  196.1493900850744\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45489895478975\n",
      "Future loss amount is:  196.14906340974886\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.454996581397115\n",
      "Future loss amount is:  196.14877213800122\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.455117073537526\n",
      "Future loss amount is:  196.1485120460172\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45525378895522\n",
      "Future loss amount is:  196.1482794549591\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45540136146779\n",
      "Future loss amount is:  196.14807115631766\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.455555482849757\n",
      "Future loss amount is:  196.14788434797322\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of loss gradient is  17.45571272063853\n",
      "Future loss amount is:  196.14771657937058\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.455870365942364\n",
      "Future loss amount is:  196.14756570445286\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.456026306340714\n",
      "Future loss amount is:  196.14742984121492\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45617891977308\n",
      "Future loss amount is:  196.1473073369051\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.456326986002352\n",
      "Future loss amount is:  196.14719673805644\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45646961279941\n",
      "Future loss amount is:  196.14709676464912\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45660617445997\n",
      "Future loss amount is:  196.1470062878093\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45673626065735\n",
      "Future loss amount is:  196.1469243105422\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45685963395617\n",
      "Future loss amount is:  196.1468499510667\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.456976194583795\n",
      "Future loss amount is:  196.1467824283854\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.457085951280597\n",
      "Future loss amount is:  196.14672104977637\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.457188997242156\n",
      "Future loss amount is:  196.14666519993972\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45728549031941\n",
      "Future loss amount is:  196.14661433156806\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45737563678324\n",
      "Future loss amount is:  196.14656795714478\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45745967806071\n",
      "Future loss amount is:  196.14652564180446\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45753787995946\n",
      "Future loss amount is:  196.14648699710685\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.457610523955115\n",
      "Future loss amount is:  196.14645167560494\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45767790020304\n",
      "Future loss amount is:  196.14641936609698\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.457740301975885\n",
      "Future loss amount is:  196.14638978947355\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.457798021287356\n",
      "Future loss amount is:  196.14636269507946\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.457851345489413\n",
      "Future loss amount is:  196.14633785752193\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45790055467774\n",
      "Future loss amount is:  196.1463150738698\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.457945919754685\n",
      "Future loss amount is:  196.14629416118845\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.457987701034664\n",
      "Future loss amount is:  196.14627495437136\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.458026147284436\n",
      "Future loss amount is:  196.1462573042271\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.458061495121417\n",
      "Future loss amount is:  196.14624107579135\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.458093968693056\n",
      "Future loss amount is:  196.14622614683444\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45812377958445\n",
      "Future loss amount is:  196.14621240654094\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.458151126901832\n",
      "Future loss amount is:  196.1461997543384\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.458176197494467\n",
      "Future loss amount is:  196.14618809885923\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.458199166282377\n",
      "Future loss amount is:  196.1461773570166\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.458220196659997\n",
      "Future loss amount is:  196.14616745318378\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.458239440956685\n",
      "Future loss amount is:  196.14615831846163\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.4582570409357\n",
      "Future loss amount is:  196.1461498900257\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.458273128315284\n",
      "Future loss amount is:  196.1461421105422\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.458287825302126\n",
      "Future loss amount is:  196.1461349276468\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.458301245128197\n",
      "Future loss amount is:  196.14612829347647\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.458313492581453\n",
      "Future loss amount is:  196.14612216425078\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.45832466452708\n",
      "Future loss amount is:  196.1461164998949\n",
      "new loss < old loss, we can increase eta\n",
      "**********\n",
      "The norm of loss gradient is  17.458334850414793\n",
      "Future loss amount is:  196.1461112637011\n",
      "MSE of gradient descent solver:  0.511557100581121\n",
      "MSE of built-in solver:  0.511716866528786\n",
      "Distance between w-coefficients:  0.006208472677174195\n"
     ]
    }
   ],
   "source": [
    "# Generate 200 data points\n",
    "n = 200\n",
    "x,y = generate_data(n)\n",
    "# Set regularization constant\n",
    "C = 10.0\n",
    "# Run gradient descent solver and compute its MSE\n",
    "w, b, losses = ridge_regression_GD(x,y,C)\n",
    "# Use built-in routine for ridge regression and compute MSE\n",
    "regr = linear_model.Ridge(alpha=C)\n",
    "regr.fit(x, y)\n",
    "# Print MSE values and L2 distance between the regression functions\n",
    "print(\"MSE of gradient descent solver: \", compute_mse(w,b,x,y))\n",
    "print(\"MSE of built-in solver: \", mean_squared_error(regr.predict(x), y))\n",
    "print(\"Distance between w-coefficients: \", np.linalg.norm(w-regr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.72922606e-01,  9.12387867e-01,  1.02857447e+00,  8.84377070e-01,\n",
       "        1.07595411e+00,  9.90560760e-01,  9.36886884e-01,  1.02024295e+00,\n",
       "        7.76572998e-01,  8.37638885e-01, -6.50122262e-02, -3.02433078e-02,\n",
       "       -6.75298913e-02, -3.13456272e-02,  2.50167334e-02,  5.79490007e-02,\n",
       "        3.19210897e-02,  7.84786716e-02, -1.64843454e-01, -1.00800972e-02,\n",
       "        2.98751389e-02,  1.51344137e-01, -3.10724276e-03,  1.93785915e-02,\n",
       "       -6.37642854e-02, -3.03598855e-02, -1.17764011e-01,  1.03975224e-01,\n",
       "       -8.56209901e-02,  1.51013160e-02, -2.48931764e-02,  1.48742885e-01,\n",
       "        1.39556106e-01,  3.35901665e-02,  1.98192881e-01,  7.70166933e-03,\n",
       "        3.82373566e-02,  8.72424578e-02, -3.18792681e-02,  3.46162181e-02,\n",
       "        1.76820745e-01, -4.42804440e-02,  1.04527445e-01,  7.40644495e-02,\n",
       "        6.48423563e-02,  1.13033422e-01, -7.44050892e-02, -8.39123461e-02,\n",
       "        4.94976731e-02,  5.67822987e-02, -7.47946042e-02, -1.43154689e-02,\n",
       "        6.55059025e-02, -1.18630491e-02, -5.22126218e-02, -1.13561682e-01,\n",
       "        5.46865452e-02,  7.06305214e-02, -5.01339298e-03, -3.22262667e-02,\n",
       "        9.04790642e-02,  2.75724429e-02,  1.01569723e-01,  1.37605534e-02,\n",
       "       -1.00261272e-01,  2.98747678e-02,  2.42217837e-01, -5.99726891e-02,\n",
       "       -2.56226388e-04,  6.41151478e-02, -8.22001328e-04, -1.16593184e-01,\n",
       "        6.52508454e-02, -1.53762789e-02,  1.36588188e-02,  5.82343780e-02,\n",
       "        7.65600609e-02, -9.59074887e-02, -1.73903728e-02, -6.59322334e-02,\n",
       "        5.24552719e-02,  2.44125080e-02,  3.31572315e-02, -7.81028852e-02,\n",
       "       -2.65073579e-02,  1.24673718e-01,  5.52310990e-02, -9.71685586e-03,\n",
       "       -1.96965468e-01, -4.88287719e-02, -1.21666231e-01,  7.13082012e-02,\n",
       "        3.07074920e-02,  1.35083973e-03,  9.75786338e-02,  1.30789585e-02,\n",
       "        9.21873644e-02, -4.85209345e-02, -1.05317481e-01,  2.60911319e-02])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"magenta\">**Something to think about**</font>\n",
    "\n",
    "The data was originally generated using a linear function in which only ten of the 100 features (the first ten) were relevant. Does the vector `w` returned by ridge regression correctly identify the relevant features?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
